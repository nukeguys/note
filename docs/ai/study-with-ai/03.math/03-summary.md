# 02. Summary

## 핵심 전제

- 머신러닝에서 학습이란 정답을 찾는 것이 아니라, 틀림을 줄이는 과정이다.
- “정확하다”는 개념은 절대적이지 않으며, 항상 비교와 개선의 대상이다.

## 반드시 기억해야 할 관점

- 손실 함수는 수학 공식이 아니라, 우리가 어떤 실패를 더 싫어하는지에 대한 가치 판단이다.
- 평균 성능이 같다면, 성능이 더 안정적으로 유지되는 모델(저분산)이 더 신뢰할 수 있다.
- 확률은 예측의 자신감이 아니라, 불확실성에 대한 정직한 표현이다.
- 좋은 모델이란 평균적으로 덜 틀리면서, 그 성능이 반복적으로 유지되는 모델이다.

## 핵심 연결 개념

- 벡터: 현실의 대상은 여러 속성의 묶음으로 표현된다.
- 평균: 전체를 판단하기 위한 기준점이다.
- 분산: 그 기준점이 얼마나 신뢰 가능한지를 나타낸다.
- 손실 함수: 틀림을 숫자로 바꿔 비교 가능하게 만든다.
- 최적화: 손실을 줄이는 방향으로 반복적으로 개선하는 과정이다.

## 대표 질문

- 같은 정확도를 가진 두 모델 중, 우리는 왜 더 안정적인 모델을 선택하는가?
- 실패의 형태가 달라지면, 왜 손실 함수도 달라져야 하는가?
- “덜 틀린다”는 말을 우리는 어떻게 수치로 증명할 수 있는가?
