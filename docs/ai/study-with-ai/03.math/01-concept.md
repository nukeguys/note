# 02. Concept Map

## Week 3의 핵심 질문 (단 하나)

> **“기계는 무엇을 기준으로 ‘잘 배웠다’고 판단하는가?”**

이 질문에 답하지 못하면,

- 모델 성능
- 일반화
- 과적합
- 최적화

는 전부 **공허한 단어**가 됩니다.

## 오늘 다룰 개념 지도 (미리 정리)

이번 주차에서 등장하는 개념들은 서로 **하나의 이야기**입니다.

1. 벡터 — “입력은 왜 숫자 묶음인가?”
2. 평균 — “왜 대표값이 필요한가?”
3. 분산 — “왜 흔들림이 문제인가?”
4. 손실 함수 — “틀림을 어떻게 숫자로 바꾸는가?”
5. 최적화 — “왜 줄이려는가?”
6. 경사하강 — “어떻게 줄이는가?”

👉 **외우지 마세요.**  
👉 **이 질문 하나로만 보세요.**

> “이게 없으면, 우리는 뭘 판단할 수 없을까?”

## 1️⃣ 벡터: 왜 입력은 ‘숫자 하나’가 아닌가

머신러닝에서 입력은 거의 항상 **벡터**입니다.

왜냐하면 현실의 대상은:

- 키 하나 ❌
- 몸무게 하나 ❌
- 나이 하나 ❌

로 표현되지 않기 때문입니다.

현실의 대상 = **여러 속성의 묶음**

그래서 모델이 보는 대상은 이렇게 됩니다.

```text
사람 = [키, 몸무게, 나이, …]
이미지 = [픽셀1, 픽셀2, 픽셀3, …]
문장 = [토큰1, 토큰2, 토큰3, …]
```

👉 벡터란 수학 개념 이전에  
👉 **“현실을 다루기 위한 최소 단위”**입니다.

## 2️⃣ 평균: 왜 ‘대표’가 필요한가

질문 하나 던질게요.

> 데이터가 100개 있으면,  
> 모델은 그 100개를 **다 기억해야 할까요?**

현실적으로:

- 기억 ❌
- 비교 ❌
- 판단 ❌

그래서 필요한 게 **대표값**입니다.

평균의 본질은 이겁니다.

> **“전체를 하나의 기준점으로 압축한다”**

중요한 포인트:

- 평균은 진실이 아닙니다
- 평균은 **판단을 가능하게 만드는 기준점**입니다

## 3️⃣ 분산: 왜 흔들림이 위험한가

평균이 같아도,

- 항상 비슷한 경우
- 들쭉날쭉한 경우

는 전혀 다릅니다.

분산이 크다는 말의 진짜 의미:

> **“이 평균을 믿어도 되는가?”**

머신러닝에서 분산은 단순 통계가 아니라:

- 일반화 실패
- 불안정한 예측
- 과적합

의 **신호**입니다.

👉 “잘 맞는다”보다  
👉 **“안정적으로 덜 틀린다”**가 중요합니다.

## 4️⃣ 손실 함수: 틀림을 숫자로 만드는 장치

이제 핵심입니다.

모델은 이렇게 묻지 않습니다.

> “이게 맞아?” ❌  
> “얼마나 틀렸어?” ⭕

손실 함수의 역할:

> **현실의 실패를 숫자로 번역한다**

예를 들어:

- 조금 틀림 → 작은 숫자
- 많이 틀림 → 큰 숫자

왜 이게 중요할까요?

> **줄이려면, 먼저 셀 수 있어야 합니다.**

## 5️⃣ 최적화: 학습의 진짜 의미

이제 정의를 하나 줍니다.  
이 정의는 외워도 됩니다.

> **머신러닝에서 학습이란  
> 손실 함수를 최소화하는 과정이다.**

지능 ❌  
이해 ❌  
의식 ❌

👉 오직 **덜 틀리기**

## 6️⃣ 경사하강: “어디로 가야 덜 틀릴까?”

경사하강을 수식으로 이해하려고 하면 바로 막힙니다.  
우리는 이렇게만 이해합니다.

> **“조금 바꿨을 때, 더 나아지는 방향으로 움직인다.”**

- 잘못된 방향 → 손실 증가
- 좋은 방향 → 손실 감소

그래서:

- 한 번에 완벽 ❌
- **조금씩 개선 ⭕**

이게 당신이 말했던 바로 그 말입니다.

> **“정확한 게 아니라 정확에 가까워진다.”**

## Week 3에서 반드시 남겨야 할 관점

이 문장은 꼭 잡고 가세요.

> **머신러닝은 답을 찾는 시스템이 아니라  
> 틀림을 관리하는 시스템이다.**

이 관점을 잡으면:

- 왜 확률인가
- 왜 100%가 불가능한가
- 왜 평가가 중요한가

가 전부 자연스럽게 이어집니다.
