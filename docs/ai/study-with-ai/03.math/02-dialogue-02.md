# 03. Dialogue Log - Step2

## 왜 오차를 ‘제곱’할까?

이 질문은 단순한 테크닉 질문이 아닙니다.  
이걸 이해하면:

- 평균을 왜 쓰는지
- 분산이 왜 위험 신호인지
- 일반화가 왜 어려운지

가 **한 번에 연결**됩니다.

## 먼저, 아주 자연스러운 의문부터

모델의 예측이 틀렸다면,  
우리는 이렇게 생각할 수 있습니다.

- 오차 = 예측 − 실제값

그럼 손실은:

- |오차| (절댓값)
- 오차² (제곱)
- 그냥 오차?

왜 굳이 **제곱**일까요?

## 1️⃣ 이유 하나: 방향을 없애기 위해서

오차에는 방향이 있습니다.

- +10 (많이 예측)
- −10 (적게 예측)

그런데 학습에서 중요한 건:

> **“얼마나 틀렸는가”**

이지,

> “어느 쪽으로 틀렸는가”

가 아닙니다.

제곱하면:

- +10² = 100
- −10² = 100

👉 방향이 사라집니다.

(절댓값도 방향은 사라집니다.  
그럼에도 제곱을 쓰는 이유는 다음입니다.)

## 2️⃣ 이유 둘: 크게 틀린 걸 더 싫어하기 위해서

이게 **핵심 이유**입니다.

예를 들어:

- 오차 1 → 손실 1
- 오차 2 → 손실 4
- 오차 3 → 손실 9

오차가 커질수록:

- 손실은 **기하급수적으로 증가**

이 말은 곧:

> **“조금 틀리는 건 참아줄 수 있지만,  
> 크게 틀리는 건 절대 용납하지 않겠다.”**

라는 **가치 판단**입니다.

## 이건 수학이 아니라 태도입니다

제곱 손실은 이렇게 말합니다.

> “평균적으로 좀 틀리는 것보다,  
> 가끔 크게 틀리는 게 더 위험하다.”

이 태도는:

- 금융
- 의료
- 안전 시스템

에서 매우 자연스럽습니다.

## 3️⃣ 이유 셋: 평균과 연결되기 때문

여기서 중요한 연결이 나옵니다.

> **제곱 오차를 최소화하는 값은  
> 항상 ‘평균’입니다.**

이건 증명으로 가면 복잡해지지만,  
직관은 단순합니다.

- 제곱 오차는
- 전체 오차를
- 가장 균형 있게 줄이는 지점

👉 그 지점이 평균입니다.

그래서:

- 평균
- 분산
- 제곱 오차

는 **한 묶음**입니다.

## 4️⃣ 그래서 분산이 위험 신호가 됩니다

분산은 이렇게 정의됩니다.

> **“평균으로부터 얼마나 제곱해서 떨어져 있는가”**

분산이 크다는 말은:

- 평균 주변에 모이지 않는다
- 예측이 들쭉날쭉하다
- 새로운 데이터에 불안정하다

👉 일반화 실패의 신호입니다.

## 다시, 당신의 말과 연결해 봅시다

당신은 이렇게 말했습니다.

> “정확한 게 아니라  
> 정확에 가까워진다”

제곱 오차는 이 말을 이렇게 구현합니다.

> **“가장 덜 위험한 중심으로 수렴하자.”**

## 아주 중요한 경고 하나

제곱 오차는 **항상 옳지 않습니다.**

- 이상치(outlier)에 민감
- 큰 오차를 과하게 벌함
- 특정 문제에 부적합

그래서:

- 분류
- 확률 예측
- 불균형 데이터

에서는 다른 손실을 씁니다.

👉 다시 한 번 강조합니다.

> **손실 함수는 수학 공식이 아니라,  
> 우리가 싫어하는 실패의 형태를 고른 것이다.**

---

## 오늘의 Dialogue 질문

이번 질문은 조금 더 날카롭습니다.

> **“만약 ‘크게 틀리는 것보다  
> 자주 조금 틀리는 게 더 위험한 문제’라면,  
> 제곱 오차는 여전히 좋은 선택일까?”**

짧아도 괜찮습니다.  
당신의 직관을 그대로 적어주세요.

👉 답변 주시면  
👉 다음 Step(왜 확률이 필요한가)로 넘어갑니다.

### 답변

> 실패의 기준이 달라지면 측정 기준도 달라진다. 따라서 제곱 오차는 좋은 선택이 아니다.

네. **정확하고, 핵심을 찔렀습니다.**  
그리고 이 한 문장은 **Week 3의 중간 결론**으로 써도 될 정도로 좋습니다.

당신의 문장을 조금만 확장해 보겠습니다.  
이미 논리는 완성돼 있습니다.

## 당신의 답변을 그대로 정제하면

당신의 문장:

> _“실패의 기준이 달라지면 측정 기준도 달라진다.”_

이건 이렇게 읽힙니다.

> **“손실 함수는 보편적 진리가 아니라  
> 문제에 대한 가치 판단이다.”**

그래서 이어지는 결론도 정확합니다.

> _“따라서 제곱 오차는 좋은 선택이 아니다.”_

→ **‘항상’ 좋은 선택이 아니다**  
가 정확한 표현입니다.

## 이게 왜 그렇게 중요한가

많은 입문자들이 이렇게 오해합니다.

- “제곱 오차 = 기본”
- “손실 함수는 정답이 있다”

이 오해를 지금 단계에서 깨셨다는 건  
상당히 앞서 있다는 뜻입니다.

## 실패의 형태가 다르면, 학습도 달라진다

아주 간단한 대비를 보죠.

### 경우 A

- 가끔 크게 틀리면 치명적  
   → 제곱 오차 ⭕

### 경우 B

- 자주 조금씩 틀리는 게 더 위험  
   → 제곱 오차 ❌

예:

- 추천 시스템
- 사용자 만족도
- 반복적 의사결정

이런 문제에서는:

- 큰 실수 1번보다
- 잦은 작은 실수가 누적 피해를 만듭니다.

## 그래서 손실 함수는 ‘정책’이다

다시 한 번 강조합니다.

> **손실 함수는  
> 모델에게 주는 ‘윤리’이자 ‘정책’이다.**

- 무엇을 용납할 것인가
- 무엇을 절대 허용하지 않을 것인가

를 수식으로 고정합니다.

## 여기서 자연스럽게 다음 질문이 나옵니다

> **“그럼 손실 함수를 어떻게 고르지?”**

이 질문의 답은:

- 수학 ❌
- 공식 ❌
- **문제 맥락 + 실패 비용**입니다.

그리고 이 질문은  
**확률**로 이어질 수밖에 없습니다.

## 다음 Step 예고

다음에는 이 질문을 다룹니다.

> **“왜 머신러닝은  
> 점수 대신 확률을 말하려 하는가?”**

이 질문 하나로:

- 분류
- 불확실성
- 신뢰도
- Calibration

이 전부 연결됩니다.
